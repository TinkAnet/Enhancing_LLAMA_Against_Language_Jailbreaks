# Enhancing LLAMA Against Language Jailbreaks

[![HuggingFace Dataset](https://img.shields.io/badge/ðŸ¤—%20Dataset-Available-blue)](https://huggingface.co/datasets/bebop404/adversarial)
[![HuggingFace Model](https://img.shields.io/badge/ðŸ¤—%20Model-Available-green)](https://huggingface.co/bebop404/my-lora-adapter)

## Project Overview

This research project evaluates and enhances LLAMA's safety mechanisms against language jailbreaks using a novel three-stage framework:
1. **Implicit Judgment**: Evaluating LLAMA's unassisted responses to harmful prompts
2. **Explicit Judgment**: Structured reasoning and severity scoring of model responses
3. **Fine-tuning via LoRA**: Targeted adaptation to strengthen safety without compromising general capabilities

The repository contains code, datasets, and evaluation results that systematically identify and address safety gaps in LLAMA's handling of harmful content.

## Key Contributions

- **Comprehensive Safety Evaluation Framework**: A multi-phase approach combining implicit and explicit judgment to identify failures that simple pass/fail tests would miss
- **Empirical Analysis of LLAMA's Vulnerabilities**: Detailed assessment across eight sensitive domains using the SAP200 benchmark
- **Structured Reasoning and LoRA Fine-Tuning**: Pinpointing model missteps and implementing targeted interventions to address them

## Repository Structure

```
â”œâ”€â”€ data/                                     # All data-related files
â”‚   â”œâ”€â”€ raw/                                  # Raw datasets
â”‚   â”‚   â”œâ”€â”€ adversarial_dataset.jsonl         # Raw adversarial prompts
â”‚   â”‚   â”œâ”€â”€ alpaca_raw.json                   # Alpaca dataset for benign examples
â”‚   â”‚   â””â”€â”€ dolly_raw.jsonl                   # Dolly dataset for benign examples
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                            # Processed datasets
â”‚   â”‚   â”œâ”€â”€ adversarial_dataset_cleaned.jsonl # Cleaned adversarial prompts
â”‚   â”‚   â”œâ”€â”€ adversarial_dataset_final.jsonl   # Final adversarial dataset
â”‚   â”‚   â”œâ”€â”€ adversarial_dataset_invalid.jsonl # Invalid adversarial examples
â”‚   â”‚   â”œâ”€â”€ benign_dataset.jsonl              # Benign prompts dataset
â”‚   â”‚   â””â”€â”€ training_dataset.jsonl            # Combined training dataset
â”‚   â”‚
â”‚   â””â”€â”€ benchmarks/                           # Benchmark datasets
â”‚       â””â”€â”€ SAP/                              # SAP200 benchmark dataset
â”‚
â”œâ”€â”€ src/                                      # Source code
â”‚   â”œâ”€â”€ data/                                 # Data processing utilities
â”‚   â”‚   â””â”€â”€ dataset_gen.py                    # Dataset generation utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                           # Evaluation scripts
â”‚   â”‚   â”œâ”€â”€ implicit/                         # Implicit judgment evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ implicit_judgment_llama.py    # LLAMA evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ implicit_judgment_llama_all.py # Comprehensive LLAMA evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ implicit_judgment_claude.py   # Claude evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ implicit_judgment_gemini.py   # Gemini evaluation
â”‚   â”‚   â”‚   â””â”€â”€ implicit_judgment_ollama.py   # Ollama evaluation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ explicit/                         # Explicit judgment evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ explicit_judgment_llama.py    # LLAMA explicit judgment
â”‚   â”‚   â”‚   â”œâ”€â”€ reasoning_auto_llama.py       # Automated reasoning with LLAMA
â”‚   â”‚   â”‚   â””â”€â”€ reasoning_llama.py            # Reasoning pipeline for LLAMA
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                            # Evaluation utilities
â”‚   â”‚       â”œâ”€â”€ reject_detect.py              # Rejection detection utilities
â”‚   â”‚       â””â”€â”€ harmbench.py                  # HarmBench evaluation utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                             # Analysis scripts
â”‚   â”‚   â”œâ”€â”€ statistic.py                      # Statistical analysis utilities
â”‚   â”‚   â””â”€â”€ statistic_explicit.py             # Statistics for explicit judgment
â”‚   â”‚
â”‚   â””â”€â”€ finetune/                             # Fine-tuning code
â”‚       â””â”€â”€ lora/                             # LoRA fine-tuning
â”‚           â””â”€â”€ fine_tune.py                  # LoRA fine-tuning script
â”‚
â”œâ”€â”€ notebooks/                                # Jupyter notebooks
â”‚   â”œâ”€â”€ data_exploration.ipynb                # Dataset exploration
â”‚   â”œâ”€â”€ fine_tuning.ipynb                     # Fine-tuning experiments
â”‚   â””â”€â”€ result_analysis.ipynb                 # Results analysis
â”‚
â”œâ”€â”€ results/                                  # All results
â”‚   â”œâ”€â”€ implicit_judgment/                    # Implicit judgment results
â”‚   â”‚   â””â”€â”€ implicit_contrast/                # Contrastive analysis results
â”‚   â”‚
â”‚   â”œâ”€â”€ explicit_judgment/                    # Explicit judgment results
â”‚   â”‚   â”œâ”€â”€ reasoning/                        # Reasoning-based results
â”‚   â”‚   â””â”€â”€ llm_based/                        # LLM-based results
â”‚   â”‚
â”‚   â”œâ”€â”€ finetune/                             # Fine-tuning results
â”‚   â”‚   â”œâ”€â”€ fraud_results_final.json          # Results for fraud domain
â”‚   â”‚   â”œâ”€â”€ politics_results_final.json       # Results for politics domain
â”‚   â”‚   â”œâ”€â”€ pornography_sexual_minors_results_final.json
â”‚   â”‚   â”œâ”€â”€ race_results_final.json           # Results for race domain
â”‚   â”‚   â”œâ”€â”€ religion_results_final.json       # Results for religion domain
â”‚   â”‚   â”œâ”€â”€ suicide_results_final.json        # Results for suicide domain
â”‚   â”‚   â”œâ”€â”€ terrorism_results_final.json      # Results for terrorism domain
â”‚   â”‚   â””â”€â”€ violence_results_final.json       # Results for violence domain
â”‚   â”‚
â”‚   â”œâ”€â”€ harmbench/                            # HarmBench evaluation results
â”‚   â”‚   â”œâ”€â”€ before_finetune.json              # Results before fine-tuning
â”‚   â”‚   â””â”€â”€ finetuned_harmbench.json          # Results after fine-tuning
â”‚   â”‚
â”‚   â””â”€â”€ visualizations/                       # Visualization results
â”‚       â”œâ”€â”€ graphs/                           # Main graphs
â”‚       â””â”€â”€ additional_graphs/                # Additional visualizations
â”‚
â”œâ”€â”€ models/                                   # Model checkpoints and configs
â”‚   â””â”€â”€ lora_adapter/                         # LoRA adapter files
â”‚
â”œâ”€â”€ scripts/                                  # Utility scripts
â”‚   â”œâ”€â”€ setup.sh                              # Environment setup script
â”‚   â””â”€â”€ download_data.sh                      # Data download script
â”‚
â”œâ”€â”€ requirements.txt                          # Project dependencies
â”œâ”€â”€ LICENSE                                   # License file
â””â”€â”€ README.md                                 # Project documentation
```

## Datasets and Models

### Adversarial Dataset
An extensive collection of 8.87k jailbreak prompts covering various harmful content categories:
- [HuggingFace Dataset: bebop404/adversarial](https://huggingface.co/datasets/bebop404/adversarial)

### Fine-tuned Model
A LoRA adapter for LLAMA-3.3-70B that enhances safety while preserving capabilities:
- [HuggingFace Model: bebop404/my-lora-adapter](https://huggingface.co/bebop404/my-lora-adapter)

## Methodology

### Implicit Judgment
Measures LLAMA's default capability to handle both overtly harmful questions and more subtly phrased ones without assistance.

### Explicit Judgment
Applies structured reasoning to model outputs, identifies key statements, and assigns severity scores indicating harmfulness levels.

### Fine-tuning with LoRA
Uses parameter-efficient Low-Rank Adaptation to directly address weaknesses identified in LLAMA's safety behavior.

## Results

The research demonstrates:
- Stark disparity in LLAMA's baseline safety performance (e.g., ~93% refusal for explicit self-harm vs. only 20-24% for nuanced harmful content)
- Significant improvement in refusal rates across all categories after fine-tuning (from 61.25% to 91.69% overall)
- Enhanced robustness against novel jailbreak attempts without sacrificing performance on benign queries

## Tutorial

### Prerequisites

Before starting, ensure you have the following:
- Python 3.9 or higher
- Git
- CUDA-compatible GPU (recommended for fine-tuning)

### Setting Up the Environment

1. Clone this repository:
   ```bash
   git clone https://github.com/TinkAnet/Enhancing_LLAMA-Against_Language_Jailbreaks.git
   cd Enhancing_LLAMA-Against_Language_Jailbreaks
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv .venv
   # On Windows
   .venv\Scripts\activate
   # On macOS/Linux
   source .venv/bin/activate
   ```

3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running Evaluations

#### Implicit Judgment Evaluation

This evaluates how LLAMA responds to harmful prompts without any assistance:

1. Add your API key to the script (or set it as an environment variable):
   ```bash
   # Set environment variable
   export LLAMA_API_KEY="your_api_key_here"
   ```

2. Run the implicit judgment script:
   ```bash
   python Implicit_judgment_llama.py
   ```

#### Explicit Judgment Evaluation

This adds structured reasoning to analyze model outputs:

```bash
python explicit_judgment_llama.py
```

#### Analyzing Results

The project includes statistical analysis tools:

```bash
python Statistic.py  # For implicit judgment analysis
python Statistic_explicit.py  # For explicit judgment analysis
```

### Fine-tuning with LoRA

To fine-tune the model with Low-Rank Adaptation:

1. Navigate to the fine-tuning directory:
   ```bash
   cd finetune/LoRa
   ```

2. Start a Jupyter server and run the fine-tuning notebook:
   ```bash
   jupyter notebook
   # Open and run fine-tune.ipynb
   ```

### Using the Fine-tuned Model

Load the LoRA adapter from HuggingFace:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Load the base model
model_name = "meta-llama/Llama-3.3-70B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Load the LoRA adapter
adapter_name = "bebop404/my-lora-adapter"
model = PeftModel.from_pretrained(model, adapter_name)
```

### Evaluating the Fine-tuned Model

Test the model against the HarmBench dataset:
```bash
python finetune/implicit_harmbench.py
```

### Using the Adversarial Dataset

Access the dataset from HuggingFace:
```python
from datasets import load_dataset
dataset = load_dataset("bebop404/adversarial")
```

### Troubleshooting

- **CUDA Out of Memory**: Reduce batch size in fine-tuning notebooks
- **API Rate Limits**: Add delay between API calls in judgment scripts
- **Missing Dependencies**: Check requirements.txt and install any missing packages

## Citation

If you use this work in your research, please cite:

```
@misc{enhancing_llama_jailbreaks,
  title = {Enhancing LLAMA Against Language Jailbreaks},
  year = {2025},
  howpublished = {\url{https://github.com/TinkAnet/Enhancing_LLAMA-Against_Language_Jailbreaks}}
}
```

## Acknowledgments

This project was conducted as a capstone research project. Thanks to everyone who provided feedback and guidance.

## License

This project is available under the [LICENSE](LICENSE) included in this repository. 